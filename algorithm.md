## 启发式算法
贪心算法的升级版
在启发式搜索里面，会考虑两个集合，一个叫**Open set** 一个叫 **Closed set**

Open：表示当前需要考虑的，还未完成搜索的节点  
Closed：表示已经处理完毕，不会再考虑的节点  
启发式搜索的大概步骤是：  

1. 把初始的节点加入到Open集合中  
2. 将Open set中的节点按照评估函数f(x)的大小从小到大排序，选出最小的那个，设置成当前的节点（CS），如果已经没有可以选的了，结束，表示未搜索到  
3. 如果CS是终点，结束操作  
4. 将CS的所有可达节点进行以下操作：  
如果节点在Closed list中，继续下一个  
否则加入到 Open list中  
将CS加入Closed list中，回到第2步    
#### 算法的评估函数为：f(n) = g(n) + h(n)    
g(n) 表示从开始移动到节点 **n** 的实际代价    
h(n) 启发式估计当前节点**n**到目标节点的代价
#### 最优解函数为：f*(n)=g*(n)+h*(n)  
g*(n):从起点到**n**的最优代价  
h*(n):从**n**到目标的最优代价  
#### 算法保证：    
当 h(n) ≤ h*(n)（可采纳）且 h(n) 满足一致性（即对于任意节点 n 及其后继 n'，有 h(n) ≤ c(n,n') + h(n')）时，A* 算法在第一次扩展节点 n 时，就有 g(n) = g*(n)，并且能找到全局最优解。  
> c(n,n') 表示n到相邻节点的距离

 ## 推理
### 五大推理规则
|规则|	形式化表示|	通俗解释|
|---|----|-----|
|取式假言推理|	P, P→Q ⊢ Q|	如果P且P蕴含Q为真，则Q为真|
|拒式假言推理|	P→Q, ¬Q ⊢ ¬P|	如果P蕴含Q，且Q为假，则P为假|
|与消除	|P∧Q ⊢ P 或 P∧Q ⊢ Q|	如果P且Q为真，则P为真，Q也为真|
|与引入	|P, Q ⊢ P∧Q	|如果P为真且Q为真，则P且Q为真|
|全称例化	|∀X p(X) ⊢ p(a)|	如果对所有X都成立，则对某个具体a也成立|

∧逻辑与  
∨逻辑或  
¬ 取反    
P→Q P=T,Q=F为F,其余时候为T   
### 合一算法
合一：判断什么样的替换可以使两个谓词表达式匹配的算法   
合一表明了两个或多个表达式在什么条件下可以称为等价的。   
替换：   
一个替换(Substitution)就是形如**{t1/x1,t2/x2,….tn/xn}**的有限集合，x1,x2.,,,xn是互不相同的个体变元，ti不同于xi, xi也不循环出现在tj中  
如：{a/x,g(y)/y,f(g(b))/z}    正确！   
        {g(y)/x,f(x)/y}       错误！  

斯柯伦标准化：去掉所有的存在量词    
(X)(彐Y)(mother(X,Y))变为(X) (mother(X,m(X))     
组合(composition) ：在合一过程中，如果先后产生S和S’的替换，那么将S中的某个元素应用S’，所产生的新的替换，称为组合。   
    如：{X/Y, W/Z}, {V/X}, {a/V, f(b)/W}      
    组合过程： {X/Y, W/Z}, {V/X}组合产生{V/Y, W/Z}   
              {V/Y, W/Z},{a/V, f(b)/W} 组合产生{a/Y, f(b)/Z}    

#### 算法
UNIFY   
```python
case    
E1, E2 或者是常元或者是空表:    %递归终止   
        If  E1 ＝ E2  then return {  };    
        else  return FAIL;    
E1是一个变元:     
        if  E1在E2中出现 then  return FAIL;    
        else  return {E2/E1};    
E2是一个变元:     
        if  E2在E1中出现 then  return FAIL;    
        else  return {E1/E2};    
其他情况:  % E1和E2都是表     
```
核心算法
```python
begin
        HE1 = E1的第一个元素;
        HE2 = E2的第一个元素;
        SUBS1 = unify (HE1, HE2);
        if (SUBS1 ＝ FAIL) then return FAIL;
        TE1 = apply (SUBS1, E1的后半部);
        TE2 = apply (SUBS1, E2的后半部);
        SUBS2 = unify (TE1, TE2 );
        if  (SUBS2 ＝ FAIL) then  return FAIL;
        else return SUBS1与SUBS2 的并集;
 end
```
example 
```python
E1:parents(X, father(X), mother(bill)) 
E2:parents(bill, father(bill), Y)  
SUBS1 = unify (HE1, HE2);   
{bill/X}  
E1:parents(bill, father(bill), mother(bill))  
E2:parents(bill, father(bill), Y)   
 SUBS2 = unify (TE1, TE2 );     
{mother(bill)/Y} 
E1:parents(bill, father(bill), mother(bill))  
E2:parents(bill, father(bill), mother(bill))   
finally  
{bill/X, mother(bill)/Y}   组合
```

### 归结

<img width="731" height="390" alt="image" src="https://github.com/user-attachments/assets/43c794ce-1962-40d8-990f-29d333e7df79" />
<img width="707" height="523" alt="image" src="https://github.com/user-attachments/assets/fa7c9b71-f258-4bb4-a25c-01e416c4107a" />


算法步骤： 
1. 转换子句
> 消去蕴含： a→b ≡﹁a∨b   
> 全称量词直接省略，因为你可以用很多个替换实现  
> 然后把所有子句换成合取范式（内层是∨外层是∧的），然后拆分成只有﹁和∨的子句
    
2. 只使用消解规则和替换进行推导，弄出证明树

## 贝叶斯网络
### 网络节点的独立性关系
#### (1) 顺序连接  
X → Y → Z
**条件独立性质**：给定Y，X和Z独立   
**直观解释**：Y是X和Z之间的中介变量。一旦Y已知，X的信息不会影响Z（反之亦然）  
**数学表达**：P(Z|X,Y) = P(Z|Y)   
**示例：**   
旧活塞(Z) → 油耗过高(Y) → 油量过低(X)   
已知油耗情况时，活塞状态与油量独立    
#### (2) 分支连接
   X ← Y → Z    
   (Y是共同原因)    

**条件独立性质**:给定Y，X和Z独立 
**直观解释**：X和Z是Y的共同结果。一旦知道共同原因Y，两个结果间不再有相互依赖 
**数学表达**：P(Z|X,Y) = P(Z|Y) 
**示例：**  
旧活塞(Y) → 蓝烟(X) ∧ 油耗过高(Z)    
已知活塞状态时，蓝烟与油耗相互独立   
#### (3) 汇合连接
X → Y ← Z  
(X、Z是Y的共同原因)   
**条件独立性质**：Y未知时，X和Z独立；Y已知时，X和Z不独立    
**直观解释**：X和Z通过"解释竞争"相互影响   
**示例：**     
油耗过高(X) → 油量过低(Y) ← 漏油(Z)    
未知油量时，油耗与漏油独立     
已知油量过低时，油耗与漏油相互影响（若已知漏油，则油耗可能性降低）   
#### 分支和汇合
   X ← Y → Z    
   X → U ← Z   
**条件独立性质**:仅给定X，U和Y不独立。当且仅当给定Z和X时，Y和U独立。   

#### d-可分性（d-separation）理论
d-可分性是系统性判断贝叶斯网络中任意节点集合间条件独立性的图论方法     
**定义**   
设A、B为两个节点集合，E为证据（条件）节点集合， A与B间的所有路径都被E堵塞，则称之为A，B独立。
**就是概率统计题**

## 马尔可夫网络
**团(Clique)**：各边所在的最大全联通子图  
**势函数**：为非负函数，表示团的一个状态
**依旧是概率统计题**
### 在马尔科夫网络中判断独立性
这玩意的算法非常简单，因为马尔科夫网络是无向图，所以在考虑已知 E 集合时，集合 X和Y 集合 是否独立，只需要考虑扣掉 E之后，集合 X和集合Y是否分属不同的联通分支就行了

## 决策树学习算法
###  ID3算法详解
选择信息增益最大的属性作为划分标准，递归构建决策树。  
核心概念：信息论基础    
**熵（Entropy）：衡量系统的不确定性** 
对于二元分类问题：  
Entropy(S) = -p⁺log₂(p⁺) - p⁻log₂(p⁻)  
对于多个的问题：  
Entropy(S) =c**∑**i=1-Pi*log2(Pi)    
这里的c表示该类目有c个取值的可能性     
**example**   
S = [9+, 5-]    
Entropy(S) = -(9/14)log₂(9/14) - (5/14)log₂(5/14) ≈ 0.940    
**信息增益**
Gain(S, A)=Entropy(S) -**∑**{v∈Values(A)}(|Sv|/|S|) Entropy(Sv)
> 其中Values(A)是属性A所有可能值的集合  
> Sv是S中属性A的值为v的子集(也就是，Sv ={s∈S|A(s)=v})    
> 第一项就是原来集合S的熵，第二项是用A分类S后熵的期望值
### C4.5算法
SplitInformation(S, A) = -**Σ**{v∈Values(A)} (|Sv|/|S|) log₂(|Sv|/|S|)  
后面自行了解，公式太复杂了  
### 感知器
**公式**：**ΔWi=c（d-signal(**∑**WiXi)）Xi**  
Wi 权值   
c是常数，表示学习率   
d是期望的输出，取值为1或-1    
sign是感知机的输出，取值为1或-1    
**结果**  
期望输出和实际输出相同，不改变权值   
实际输出为-1，期望输出为+1，则增加2cXi   
实际输出为+1，期望输出为-1，则减少2cXi    

假设初始权值为𝑤0=[0,0,0], c=0.5，假设没有偏置项，sign的阈值为0（>0为1，<=0为-1），  
**以样本（111:1）为例**  
1.输入：x = (1,1,1), 期望输出 d = +1
2. net = w₀·x = [0,0,0]·[1,1,1] = 0×1 + 0×1 + 0×1 = 0  --->    
   y = sign(net) = sign(0) = -1  # 注意：sign(0) = -1（因为 ≤ 0）
3. **判断是否误分类：**
> 实际输出 y = -1   
> 期望输出 d = +1    
> y ≠ d → 误分类

符合规则："实际输出为-1，期望输出为+1，则增加2cXᵢ"  

4.**计算更新量**
Δw = 2c × x = 2 × 0.5 × [1,1,1] = 1 × [1,1,1] = [1,1,1]  
5. **更新权重：**
w₁ = w₀ + Δw = [0,0,0] + [1,1,1] = [1,1,1]

**所以得**对样本（111；1），f(A1)=-1，期望输出为+1，权值增加2cA1，则𝑤1 =[1,1,1]     
对下面的样本进行同样的操作即可得出   
对样本（110；1），f(A2)=1，期望输出为+1，权值不变；   
对样本（011；1），f(A3)=1，期望输出为+1，权值不变；        
对样本（000；0），f(A4)=-1, 期望输出为-1，权值不变；      
最后的权值为     
[1,1,1 ]。按所设置的初始值，合理即可。      

## 遗传算法
**一、遗传（选择）操作**   
遗传操作又称为选择操作，目的是从当前种群中选出适应性强的个体进入下一代。  
**步骤：**   
1. 假设的表示
2. 选择适应度高的
3. 进行交叉
4. 最后在进行变异得到最优解


题目：   
请用遗传算法求函数$𝑓（𝑥, 𝑦)=(𝑥+1)^2+(𝑦−3)^2$的极小值，考虑𝑥和𝑦的取值范围均为0-31。给出编码方法，适应度函数，软色体的选择策略，并随机初始化软色体后给出进化两代后的种群，给出进化中交叉以及变异的流程和步骤。     
参考习题       
答案：     
采用5位0，1编码x，采用5位0，1编码y     
随机初始化一些种群，可以初始化4个，采用目标函数的反作为适应度函数    
执行交叉变异操作。    
例如：初始化种群，    
第一个染色体：00010，00011；适应度函数值=-[9+0]=-9     
第二个染色体：00011，11000；适应度函数值=-[16+441]=-457     
第三个染色体：11000，10111；适应度函数值=-[289+400]=-689     
第四个染色体：00111，11000；适应度函数值=-[64+441]-505     
选取适应度最高的两个染色体（第一和第二个染色体）执行交叉，交叉点随机选择假设为4，形成00011、11000，并执行一次随机变异，假设变异点为第6位，形成00011、01000，替换适应度最低的一个染色体（第三个染色体）。     
形成的第二代染色体为：     
第一个染色体：00010，00011；适应度函数值=-[9+0]=-9    
第二个染色体：00011，11000；适应度函数值=-[16+441]=-457     
第三个染色体：00011，01000；适应度函数值=-[16+25]=-41     
第四个染色体：00111，11000；适应度函数值=-[64+441]-505。    

## 强化学习  
### MDP模型
**S**:状态空间（state space）。可以是有限集（离散）或连续空间。   
s∈S必须满足马尔可夫性：给定当前状态与当前动作，下一状态的概率分布与更早历史条件独立，即   
P(St+1​∣St​,At​,St−1​,At−1​,…)=P(St+1​∣St​,At​).

**A**:动作空间（action space），也可能依状态而异 𝐴(𝑠)。   
对每个状态 s，动作集合 𝐴(𝑠)定义 agent 可采取的选择 𝑎。动作可以是：   
>- 确定性动作:例如{up,down,left,right}
>- 连续动作：例如例如油门踏板输出 𝑎∈𝑅   

**P**：转移概率（transition probability kernel）。对于离散情形，     
𝑃(𝑠′∣𝑠,𝑎)=Pr⁡(𝑆𝑡+1=𝑠′∣𝑆𝑡=𝑠,𝐴𝑡=𝑎)这一项反映环境的动力学（model）。    
**𝑅**:即时奖励函数（reward）。常有两种表述：     
>- 确定性： 𝑟(𝑠,𝑎,𝑠′)；   
>- 随机性： 𝑅𝑡+1的期望 𝐸[𝑅𝑡+1∣𝑠,𝑎]
 
**γ∈[0,1]**：折扣因子（discount factor），用于权衡远期奖励。   
若 γ=0：完全短视，只优化即时奖励。𝛾->1重视长期回报,对于无限期任务，需要 𝛾<1保证折扣和收敛，或者限制为 episodic；𝛾=1常用于有限轨迹（episodic）问题。   

### 动态规划
给定一个完全已知的MDP模型   
>- 策略评估(Policy Evaluation)给定一个策略π, 评估其返回值    
>- 最优控制(Optimal Control)寻找一个最优策略π * (从任一状态出发，其返回值都为最大)
>- Vπ(s)： 从s状态出发，采用π策略，所获得的期望返回值   
>- Qπ(s,a)：从s状态出发， 采用a动作，继而采用π策略，所获得的期望返回值   
>- 最优值函数V*(s) and Q*(s,a) ：采用最优策略π *所获得的期望返回值

定理：策略π 为最优策略当且仅当，在每一个状态s   
V*(s) = **max**πV^π(s) Vπ(s) = **max**aQ^π(s,a)    
Bellman方程：   
𝑉^𝜋 (𝑠)=𝐸_(𝑠′~𝜋(𝑠) ) [𝑅(𝑠,𝜋(𝑠))+𝛾𝑉^𝜋 (𝑠')]
等价于：𝑉^𝜋(𝑠)=𝐸[𝑅(𝑠,𝜋(𝑠))]+𝛾∑_(𝑠′)𝛿(𝑠,𝜋(𝑠),𝑠′）𝑉^𝜋 (𝑠′)
## 博弈论 
### 纳什均衡
在非合作博弈中，一个策略组合是纳什均衡，当没有玩家能通过单方面改变策略获得更高收益     
**定义**
一个纳什均衡𝒂∗=(𝑎1∗,𝑎2∗,…,𝑎𝑁∗ )满足对任何 𝑖∈𝑁    
𝑢𝑖(𝑎𝑖∗, 𝑎−𝑖∗ )≥𝑢𝑖 (𝑎𝑖, 𝑎−𝑖∗ ) for all 𝑎𝑖∈𝐴𝑖     
这里面的  𝑎−𝑖∗ 表示除了i之外都选择纳什均衡的动作    
博弈矩阵中如何求解纳什均衡？   
1. 写出收益矩阵  
例如囚徒困境：
| 选择|坦白（P2）|	否认（P2）|
|-----|----|-----|
|坦白（P1）|(-3, -3)|	(0, -5)|
|否认（P1）|	(-5, 0)	|(-1, -1)|
2. 找出每个玩家的最优反应
对于P1：   
若P2选“坦白”，P1的最佳反应是“坦白”（-3 > -5）    
若P2选“否认”，P1的最佳反应是“坦白”（0 > -1）   
→ P1总是选择“坦白”     
对于P2：同理，P2也总是选择“坦白”
3. 寻找最优反应的交叉点    
当P1和P2都选择“坦白”时，彼此都是最优反应，这就是纯策略纳什均衡。

### 帕累托最优和帕累托改进  
**帕累托改进定义**：在不损害任何一个人的收益的前提下，使得至少一部分的人的收益增加。   
**帕累托最优**：就是没有帕累托改进空间的局面。    






 












  











